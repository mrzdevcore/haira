[package]
name = "haira-local-ai"
version.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true
authors.workspace = true
description = "Local AI backend for Haira using llama.cpp"

[dependencies]
# HTTP client
reqwest = { version = "0.11", features = ["json", "stream"] }
tokio = { workspace = true }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }

# Error handling
thiserror = { workspace = true }

# Logging
tracing = { workspace = true }

# For downloading models
sha2 = "0.10"
hex = "0.4"
futures-util = "0.3"
indicatif = "0.17"

# Platform-specific paths
dirs = "5.0"

# Unix signal handling
[target.'cfg(unix)'.dependencies]
libc = "0.2"
